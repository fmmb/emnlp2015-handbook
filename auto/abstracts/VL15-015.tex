Recurrent neural networks (RNN) have gained a reputation for beating state-of-the-art results on many NLP benchmarks and for producing rep- resentations of words and larger linguistic units that encode complex syntactic and semantic struc- tures. However, it is not straight-forward to un- derstand how exactly these models make their de- cisions. Recently Li et al. (2015) developed meth- ods to provide linguistically motivated analysis for RNNs trained for sentiment analysis. Here we fo- cus on the analysis of a multi-modal Gated Recur- rent Neural Network (GRU) architecture trained to predict image-vectors - extracted from images us- ing a CNN trained on ImageNet - from their cor- responding descriptions. We propose two meth- ods to explore the importance of grammatical cat- egories with respect to the model and the task. We observe that the model pays most attention to head-words, noun subjects and adjectival modi- fiers and least to determiners and coordinations.
