The automatic generation of image captions has received considerable attention. The problem of evaluating caption generation system,though, has not been that much explored. We propose a novel approach for evaluation based on comparing the underlaying visual semantics of the candidate caption against the semantics of the ground-truth captions. With this goal in mind we have defined a semantic representation of visually descriptive language and have augmented a subset of the Flickr-8K data-set with semantic annotations. Our evaluation metric (BAST) can be used not only to compare systems but also to do error analysis and get a better understanding of the type of mistakes a system does. To compute BAST we need to predict the semantic representation for the automatically generated captions. We use the Flickr-ST data-set to train classifiers that predict STs so that that evaluation can be fully automated.
