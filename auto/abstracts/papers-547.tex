Neural networks have been shown to improve performance across a range of natural-language tasks while addressing some issues with traditional models such as size. However, designing and training them can be complicated. Frequently, researchers resort to repeated experimentation across a range of parameters to pick optimal settings. In this paper, we address the issue of choosing the correct number of units in the hidden layers. We introduce a method for automatically adjusting network size by pruning out hidden units through $\ell\_{\infty,1}$ and $\ell\_{2,1}$ regularization. We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.
