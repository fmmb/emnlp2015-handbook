Distributed word representations are very useful for capturing semantic information and have been successfully applied in many NLP tasks, especially on English. In this work, we innovatively develop two component-enhanced Chinese character embedding models and their bi-gram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models.
