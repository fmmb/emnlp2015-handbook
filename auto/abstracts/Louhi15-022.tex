We present a distributional approach to the problem of inducing parameters for un- seen words in probabilistic parsers. Our KNN-based algorithm uses distributional similarity over an unlabelled corpus to match unseen words to the most similar seen words, and can induce parameters for those unseen words without retrain- ing the parser. We apply this to domain adaptation for three different parsers that employ fine-grained syntactic categories, which allows us to focus on modifying the lexicon, while leaving the structure of the parser itself intact. We demonstrate up- lifts for dependency recovery of 2\%-6\% on novel vocabulary in biomedical text.
