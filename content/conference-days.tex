
\chapter{Main Conference: Saturday, September 19}

\thispagestyle{emptyheader}


\section*{Overview}

\input{auto/papers/Saturday-overview.tex}

\clearpage{}


\section{Invited Speaker: Yoshua Bengio}

\index{Bengio, Yoshua}

\begin{center}
\textbf{\Large{}Deep Learning of Semantic Representations}{\Large{}\vspace{1em}
}
\par\end{center}{\Large \par}

\begin{center}
Saturday, September 19, 2015,  \vspace{1em}
\\
 \PlenaryLoc \\
 \vspace{1em}

\par\end{center}

\noindent \textbf{Abstract:} The core ingredient of deep learning
is the notion of distributed representation. This talk will start
by explaining its theoretical advantages, in comparison with non-parametric
methods based on counting frequencies of occurrence of observed tuples
of values (like with n-grams). The talk will then explain how having
multiple levels of representation, i.e., depth, can in principle give
another exponential advantage. Neural language models have been extremely
successful in recent years but extending their reach from language
modeling to machine translation is very appealing because it forces
the learned intermediate representations to capture meaning, and we
found that the resulting word embeddings are qualitatively different.
Recently, we introduced the notion of attention-based encoder-decoder
systems, with impressive results on machine translation several language
pairs and for mapping an image to a sentence, and these results will
conclude the talk.

\vspace{3em}


\vfill{}


\noindent \textbf{Biography:} Yoshua Bengio received a PhD in Computer
Science from McGill University, Canada in 1991. After two post-doctoral
years, one at M.I.T. with Michael Jordan and one at AT\&T Bell Laboratories
with Yann LeCun and Vladimir Vapnik, he became professor at the Department
of Computer Science and Operations Research at Université de Montréal.
He is the author of two books and more than 200 publications, the
most cited being in the areas of deep learning, recurrent neural networks,
probabilistic learning algorithms, natural language processing and
manifold learning. He is among the most cited Canadian computer scientists
and is or has been associate editor of the top journals in machine
learning and neural networks. Since '2000 he holds a Canada Research
Chair in Statistical Learning Algorithms, since '2006 an NSERC Industrial
Chair, since '2005 his is a Senior Fellow of the Canadian Institute
for Advanced Research and since 2014 he co-directs its program focused
on deep learning. He is on the board of the NIPS foundation and has
been program chair and general chair for NIPS. He has co-organized
the Learning Workshop for 14 years and co-created the new International
Conference on Learning Representations. His current interests are
centered around a quest for AI through machine learning, and include
fundamental questions on deep learning and representation learning,
the geometry of generalization in high-dimensional spaces, manifold
learning, biologically inspired learning algorithms, and challenging
applications of statistical machine learning.

\clearpage{}

\input{auto/papers/Saturday-Session-1.tex}
\input{auto/papers/Saturday-Session-1-abstracts.tex}
\input{auto/papers/Saturday-Session-2.tex}
\input{auto/papers/Saturday-Session-2-abstracts.tex}
\input{auto/papers/Saturday-Session-3.tex}
\input{auto/papers/Saturday-Session-3-abstracts.tex}


\chapter{Main Conference: Sunday, September 20}


\section*{Overview}

\input{auto/papers/Sunday-overview.tex}

\clearpage{}


\section{Invited Speaker: Justin Grimmer}

\index{Bengio, Yoshua}

\begin{center}
\textbf{\Large{}Measuring How Elected Officials and Constituents Communicate}{\Large{}\vspace{1em}
}
\par\end{center}{\Large \par}

\begin{center}
Sunday, September 20, 2015,  \vspace{1em}
\\
 \PlenaryLoc \\
 \vspace{1em}

\par\end{center}

\noindent \textbf{Abstract:} This talk will show how elected officials
use communication to cultivate support with constituents, how constituents
express their views to elected officials, and why biases in both kinds
of communication matter for political representation. To demonstrate
the bias and its effects, I propose to use novel collections of political
texts and new text as data methods. Using the new data and methods,
I will show how the incentives of communication contribute to perceptions
of an angry public and vitriolic politicians. Among elected officials,
the ideologically extreme members of Congress disproportionately participate
in policy debates, resulting in political debates that occur between
the most extreme members of each party. Among constituents, the most
ideologically extreme and angry voters disproportionately contact
their member of Congress, creating the impression of a polarized and
vitriolic public. The talk will explain how the findings help us to
understand how representation occurs in American politics, while also
explaining how computational tools can help address questions in the
social sciences.

\vspace{3em}


\vfill{}


\noindent \textbf{Biography:} Justin Grimmer is an associate professor
of political science at Stanford University. His research examines
how representation occurs in American politics using new statistical
methods. His first book Representational Style in Congress: What Legislators
Say and Why It Matters (Cambridge University Press, 2013) shows how
senators define the type of representation they provide constituents
and how this affects constituents' evaluations and was awarded the
2014 Richard Fenno Prize. His second book The Impression of Influence:
How Legislator Communication and Government Spending Cultivate a Personal
Vote (Princeton University Press, 2014 with Sean J. Westwood and Solomon
Messing) demonstrates how legislators ensure they receive credit for
government actions. His work has appeared in the American Political
Science Review, American Journal of Political Science, Journal of
Politics, Political Analysis, Proceedings of the National Academy
of Sciences, Regulation and Governance, and Poetics.

\clearpage{}

\input{auto/papers/Sunday-Session-4.tex}
\input{auto/papers/Sunday-Session-4-abstracts.tex}
\input{auto/papers/Sunday-Session-5.tex}
\input{auto/papers/Sunday-Session-5-abstracts.tex}
\input{auto/papers/Sunday-Session-6.tex}
\input{auto/papers/Sunday-Session-6-abstracts.tex}


\chapter{Main Conference: Monday, September 21}


\section*{Overview}

\input{auto/papers/Monday-overview.tex}

\clearpage{}

\input{auto/papers/Monday-Session-7.tex}
\input{auto/papers/Monday-Session-7-abstracts.tex}
\input{auto/papers/Monday-Session-8.tex}
\input{auto/papers/Monday-Session-8-abstracts.tex}
\input{auto/papers/Monday-Session-9.tex}
\input{auto/papers/Monday-Session-9-abstracts.tex}
