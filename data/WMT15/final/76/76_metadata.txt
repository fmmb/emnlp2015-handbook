SubmissionNumber#=%=#76
FinalPaperTitle#=%=#Multi-level Evaluation for Machine Translation
ShortPaperTitle#=%=#Multi-level Evaluation for Machine Translation
NumberOfPages#=%=#5
CopyrightSigned#=%=#NA
JobTitle#==#
Organization#==#
Abstract#==#Translations generated by current statistical often have a large variance, in
terms of their quality against human references. To cope with such variation,
we propose to evaluate translations using a multi-level framework. The method
varies the evaluation criteria based on the clusters to which a translation
belongs. Our experiments on the WMT metric task data show that the multi-level
framework consistently improves the performance of two benchmarking metrics,
resulting in better correlation with human judgment.
Author{1}{Firstname}#=%=#Boxing
Author{1}{Lastname}#=%=#Chen
Author{1}{Email}#=%=#boxing.chen@cnrc-nrc.gc.ca
Author{1}{Affiliation}#=%=#NRC
Author{2}{Firstname}#=%=#Hongyu
Author{2}{Lastname}#=%=#Guo
Author{2}{Email}#=%=#Hongyu.Guo@nrc-cnrc.gc.ca
Author{2}{Affiliation}#=%=#NRC
Author{3}{Firstname}#=%=#Roland
Author{3}{Lastname}#=%=#Kuhn
Author{3}{Email}#=%=#roland.kuhn@cnrc-nrc.gc.ca
Author{3}{Affiliation}#=%=#National Research Council of Canada

==========