SubmissionNumber#=%=#43
FinalPaperTitle#=%=#Investigations on Phrase-based Decoding with Recurrent Neural Network Language and Translation Models
ShortPaperTitle#=%=#Investigations on Phrase-based Decoding with Recurrent Neural Network Language and Translation Models
NumberOfPages#=%=#10
CopyrightSigned#=%=#Tamer Alkhouli
JobTitle#==#Research Assistant
Organization#==#Chair of Computer Science 6
RWTH Aachen University
Ahornstr. 55
52056 Aachen
Germany
Abstract#==#This work explores the application of recurrent neural network (RNN) language
and translation models during phrase-based decoding. Due to their use of
unbounded context, the decoder integration of RNNs is more challenging compared
to the integration of feedforward neural models. In this paper, we apply
approximations and use caching to enable RNN decoder integration, while
requiring reasonable memory and time resources. We analyze the effect of
caching on translation quality and speed, and use it to integrate RNN language
and translation models into a phrase-based decoder. To the best of our
knowledge, no previous work has discussed the integration of RNN translation
models into phrase-based decoding. We also show that a special RNN can be
integrated efficiently without the need for approximations.  We compare
decoding using RNNs to rescoring n-best lists on two tasks: IWSLT 2013
German->English, and  BOLT Arabic->English. We demonstrate that the performance
of decoding with RNNs is at least as good as using them in rescoring.
Author{1}{Firstname}#=%=#Tamer
Author{1}{Lastname}#=%=#Alkhouli
Author{1}{Email}#=%=#alkhouli@cs.rwth-aachen.de
Author{1}{Affiliation}#=%=#RWTH Aachen University
Author{2}{Firstname}#=%=#Felix
Author{2}{Lastname}#=%=#Rietig
Author{2}{Email}#=%=#felix.rietig@rwth-aachen.de
Author{2}{Affiliation}#=%=#RWTH Aachen University
Author{3}{Firstname}#=%=#Hermann
Author{3}{Lastname}#=%=#Ney
Author{3}{Email}#=%=#ney@cs.rwth-aachen.de
Author{3}{Affiliation}#=%=#RWTH Aachen University

==========