SubmissionNumber#=%=#16
FinalPaperTitle#=%=#Approximation-Aware Dependency Parsing by Belief Propagation
ShortPaperTitle#=%=#Approximation-Aware Dependency Parsing by Belief Propagation
NumberOfPages#=%=#14
CopyrightSigned#=%=#Matthew R. Gormley
JobTitle#==#
Organization#==#Johns Hopkins University
3400 N. Charles St.
Baltimore, MD 21218
Abstract#==#We show how to train the fast dependency parser of Smith and Eisner (2008) for
improved accuracy. This parser can consider higher-order interactions among
edges while retaining O(n^3) runtime. It outputs the parse with maximum
expected recallâ€”but for speed, this expectation is taken under a posterior
distribution that is constructed only approximately, using loopy belief
propagation through structured factors. We show how to adjust the model
parameters to compensate for the errors introduced by this approximation, by
following the gradient of the actual loss on training data. We find this
gradient by backpropagation. That is, we treat the entire parser
(approximations and all) as a differentiable circuit, as others have done for
loopy CRFs (Domke, 2010; Stoyanov et al., 2011; Domke, 2011; Stoyanov and
Eisner, 2012). The resulting parser obtains higher accuracy with fewer
iterations of belief propagation than one trained by conditional
log-likelihood.
Author{1}{Firstname}#=%=#Matthew R.
Author{1}{Lastname}#=%=#Gormley
Author{1}{Email}#=%=#mrg@cs.jhu.edu
Author{1}{Affiliation}#=%=#Johns Hopkins University
Author{2}{Firstname}#=%=#Mark
Author{2}{Lastname}#=%=#Dredze
Author{2}{Email}#=%=#mdredze@cs.jhu.edu
Author{2}{Affiliation}#=%=#Johns Hopkins University
Author{3}{Firstname}#=%=#Jason
Author{3}{Lastname}#=%=#Eisner
Author{3}{Email}#=%=#jason@cs.jhu.edu
Author{3}{Affiliation}#=%=#Johns Hopkins University

==========