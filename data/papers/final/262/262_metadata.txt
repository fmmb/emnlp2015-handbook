SubmissionNumber#=%=#262
FinalPaperTitle#=%=#Do Multi-Sense Embeddings Improve Natural Language Understanding?
ShortPaperTitle#=%=#Do Multi-Sense Embeddings Improve Natural Language Understanding?
NumberOfPages#=%=#11
CopyrightSigned#=%=#J.L
JobTitle#==#
Organization#==#Jiwei Li
Stanford University
Abstract#==#Learning a distinct representation for each sense of an ambiguous word 
could lead to more powerful and fine-grained models of
vector-space representations. Yet while `multi-sense' methods have been
proposed
and tested on artificial word-similarity tasks,
we don't know if they improve real natural language understanding tasks.
In this paper we introduce 
a multi-sense embedding model based on Chinese Restaurant Processes
that achieves state of the art performance on matching 
human word similarity judgments, and propose
a pipelined architecture for
incorporating multi-sense embeddings into language understanding.

We then test the performance of our
model on part-of-speech tagging, named
entity recognition, sentiment analysis, semantic relation identification and
semantic relatedness,
controlling for embedding dimensionality.
We find that multi-sense embeddings do improve performance on
some tasks (part-of-speech tagging, semantic relation identification, semantic
relatedness)
but not on others (named entity recognition, various forms of sentiment
analysis).
We discuss how these differences may be caused by the different role
of word sense information in each of the tasks.  The results highlight the
importance of testing embedding models
in real applications.
Author{1}{Firstname}#=%=#Jiwei
Author{1}{Lastname}#=%=#Li
Author{1}{Email}#=%=#jiweil@stanford.edu
Author{1}{Affiliation}#=%=#Stanford University
Author{2}{Firstname}#=%=#Dan
Author{2}{Lastname}#=%=#Jurafsky
Author{2}{Email}#=%=#jurafsky@stanford.edu
Author{2}{Affiliation}#=%=#Stanford University

==========