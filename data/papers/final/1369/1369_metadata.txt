SubmissionNumber#=%=#1369
FinalPaperTitle#=%=#An Empirical Analysis of Optimization for Max-Margin NLP
ShortPaperTitle#=%=#An Empirical Analysis of Optimization for Max-Margin NLP
NumberOfPages#=%=#7
CopyrightSigned#=%=#Jonathan Kummerfeld
JobTitle#==#
Organization#==#Computer Science Division, UC Berkeley, Sutardja Dai Hall, Berkeley, CA 94720
Abstract#==#Despite the convexity of structured max-margin objectives (Taskar et al., 2004,
Tsochantaridis et al., 2004), the many ways to optimize them are not equally
effective in practice. We compare a range of online optimization methods over a
variety of structured NLP tasks (coreference, summarization, parsing, etc) and
find several broad trends. First, margin methods do tend to outperform both
likelihood and the perceptron. Second, for max-margin objectives, primal
optimization methods are often more robust and progress faster than dual
methods. This advantage is most pronounced for tasks with dense or
continuous-valued features. Overall, we argue for a particularly simple online
primal subgradient descent method that, despite being rarely mentioned in the
literature, is surprisingly effective in relation to its alternatives.
Author{1}{Firstname}#=%=#Jonathan K.
Author{1}{Lastname}#=%=#Kummerfeld
Author{1}{Email}#=%=#jkk@cs.berkeley.edu
Author{1}{Affiliation}#=%=#UC Berkeley
Author{2}{Firstname}#=%=#Taylor
Author{2}{Lastname}#=%=#Berg-Kirkpatrick
Author{2}{Email}#=%=#tberg@eecs.berkeley.edu
Author{2}{Affiliation}#=%=#UC Berkeley
Author{3}{Firstname}#=%=#Dan
Author{3}{Lastname}#=%=#Klein
Author{3}{Email}#=%=#klein@cs.berkeley.edu
Author{3}{Affiliation}#=%=#UC Berkeley

==========