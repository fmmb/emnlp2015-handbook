SubmissionNumber#=%=#684
FinalPaperTitle#=%=#Posterior calibration and exploratory analysis for natural language processing models
ShortPaperTitle#=%=#Posterior calibration and exploratory analysis for natural language processing models
NumberOfPages#=%=#12
CopyrightSigned#=%=#Brendan OConnor
JobTitle#==#
Organization#==#University of Massachusetts Amherst
College of Information and Computer Sciences
Computer Science Building, 140 Governors Drive
Amherst, MA 01003-9264)
Abstract#==#Many models in natural language processing define probabilistic distributions
over linguistic structures.  We argue that (1) the quality of a model's
posterior distribution can and should be directly evaluated, as to whether
probabilities correspond to empirical frequencies; and (2) NLP uncertainty can
be projected not only to pipeline components, but also to exploratory data
analysis, telling a user when to trust and not trust the NLP analysis.
We present a method to analyze calibration, and apply it to compare the
miscalibration of several commonly used models.  We also contribute a
coreference sampling algorithm that can create confidence intervals for a
political event extraction task.
Author{1}{Firstname}#=%=#Khanh
Author{1}{Lastname}#=%=#Nguyen
Author{1}{Email}#=%=#nguyenxuankhanhm@gmail.com
Author{1}{Affiliation}#=%=#University of Maryland, College Park
Author{2}{Firstname}#=%=#Brendan
Author{2}{Lastname}#=%=#O'Connor
Author{2}{Email}#=%=#brenocon@cs.umass.edu
Author{2}{Affiliation}#=%=#Univ. of Massachusetts, Amherst

==========