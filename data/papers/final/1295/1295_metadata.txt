SubmissionNumber#=%=#1295
FinalPaperTitle#=%=#A Binarized Neural Network Joint Model for Machine Translation
ShortPaperTitle#=%=#A Binarized Neural Network Joint Model for Machine Translation
NumberOfPages#=%=#6
CopyrightSigned#=%=#Jingyi Zhang
JobTitle#==#
Organization#==#National Institute of Information and Communications Technology,
3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan
Abstract#==#The neural network joint model (NNJM),
which augments the neural network Language
model (NNLM) with an m-word
source context window, has achieved large
gains in machine translation accuracy, but
also has problems with high normalization
cost when using large vocabularies. Training
the NNJM with noise-contrastive estimation
(NCE), instead of standard maximum
likelihood estimation (MLE), can
reduce computation cost. In this paper,
we propose an alternative to NCE, the binarized
NNJM (BNNJM), which learns a
binary classifier that takes both the context
and target words as input, and can be
efficiently trained using MLE. We compare
the BNNJM and NNJM trained by
NCE on Chinese-to-English and Japanese-to-English 
translation tasks.
Author{1}{Firstname}#=%=#Jingyi
Author{1}{Lastname}#=%=#Zhang
Author{1}{Email}#=%=#zhangjingyizz@gmail.com
Author{1}{Affiliation}#=%=#NAIST&NICT
Author{2}{Firstname}#=%=#Masao
Author{2}{Lastname}#=%=#Utiyama
Author{2}{Email}#=%=#mutiyama@nict.go.jp
Author{2}{Affiliation}#=%=#NICT
Author{3}{Firstname}#=%=#Eiichiro
Author{3}{Lastname}#=%=#Sumita
Author{3}{Email}#=%=#eiichiro.sumita@nict.go.jp
Author{3}{Affiliation}#=%=#NICT
Author{4}{Firstname}#=%=#Graham
Author{4}{Lastname}#=%=#Neubig
Author{4}{Email}#=%=#neubig@is.naist.jp
Author{4}{Affiliation}#=%=#NAIST
Author{5}{Firstname}#=%=#Satoshi
Author{5}{Lastname}#=%=#Nakamura
Author{5}{Email}#=%=#s-nakamura@is.naist.jp
Author{5}{Affiliation}#=%=#NAIST

==========