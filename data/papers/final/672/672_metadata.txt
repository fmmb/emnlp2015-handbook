SubmissionNumber#=%=#672
FinalPaperTitle#=%=#Effective Approaches to Attention-based Neural Machine Translation
ShortPaperTitle#=%=#Effective Approaches to Attention-based Neural Machine Translation
NumberOfPages#=%=#10
CopyrightSigned#=%=#Thang Luong
JobTitle#==#
Organization#==#Stanford University, Computer Science Department Stanford University, Stanford, CA, 94305
Abstract#==#An attentional mechanism has lately been used to improve neural machine
translation (NMT) by selectively focusing on parts of the source sentence
during
translation. However, there has been little work exploring useful architectures
for attention-based NMT. This paper examines two simple and effective classes
of
attentional mechanism: a global approach which always attends to all source
words and a local one that only looks at a subset of source words at a time. We
demonstrate the effectiveness of both approaches over the WMT translation tasks
between English and German in both directions. With local attention, we achieve
a significant gain of 5.0 BLEU points over non-attentional systems which
already
incorporate known techniques such as dropout. Our ensemble model using
different
attention architectures has established a new state-of-the-art result in the
WMT'15 English to German translation task with 25.9 BLEU points, an improvement
of 1.0 BLEU points over the existing best system backed by NMT and an n-gram
reranker.
Author{1}{Firstname}#=%=#Thang
Author{1}{Lastname}#=%=#Luong
Author{1}{Email}#=%=#luong.m.thang@gmail.com
Author{1}{Affiliation}#=%=#Stanford University
Author{2}{Firstname}#=%=#Hieu
Author{2}{Lastname}#=%=#Pham
Author{2}{Email}#=%=#hyhieu@cs.stanford.edu
Author{2}{Affiliation}#=%=#Stanford University
Author{3}{Firstname}#=%=#Christopher D.
Author{3}{Lastname}#=%=#Manning
Author{3}{Email}#=%=#manning@cs.stanford.edu
Author{3}{Affiliation}#=%=#Stanford University

==========